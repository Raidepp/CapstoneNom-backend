{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProjectNom - Iter3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1MGj2WwexWTlp_mort6-K9bG8JivaT3ar",
      "authorship_tag": "ABX9TyPOugknBvObyMUkfE343lXT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raidepp/CapstoneNom-backend/blob/main/ProjectNom_Iter3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VbWwixvJlvCF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r ~/.kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp ./kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "1hG2LUIll16L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d kmader/food41\n",
        "!unzip food41.zip"
      ],
      "metadata": {
        "id": "GOQPRndxl18A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_h5 = 'food_c101_n10099_r64x64x3.h5'\n",
        "test_h5 = 'food_test_c101_n1000_r64x64x3.h5'\n",
        "\n",
        "fl = h5py.File(test_h5, 'r')\n",
        "test_img = fl['images']\n",
        "\n",
        "#print(test_img[0])"
      ],
      "metadata": {
        "id": "zyQGffMdl19n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating DataGenerator"
      ],
      "metadata": {
        "id": "3wF0kHdql_3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from shutil import copytree, rmtree\n",
        "\n",
        "# # Try using few labels\n",
        "# def create_subset_func(ls, dest):\n",
        "#   if os.path.exists(dest):\n",
        "#     rmtree(dest)\n",
        "#   os.makedirs(dest)\n",
        "\n",
        "#   for item in ls:\n",
        "#     # Copying all images in picked list into new directory\n",
        "#     copytree( os.path.join('images', item), os.path.join(dest, item) )\n",
        "\n",
        "# pick_label = ['donuts', 'carrot_cake', 'tiramisu', 'fried_rice', 'french_fries', 'omelette',\n",
        "#               'takoyaki', 'hot_dog', 'creme_brulee', 'chocolate_mousse', 'ramen', ]\n",
        "\n",
        "# pick_label = ['donuts', 'tiramisu', 'creme_brulee', 'chocolate_mousse' ]  # 'carrot_cake', \n",
        "# pick_label = ['pizza', 'omelette', 'apple_pie']\n",
        "# pick_label = ['apple_pie', 'beef_carpaccio', 'bibimbap', 'cup_cakes', 'foie_gras', \n",
        "#               'french_fries', 'garlic_bread', 'pizza', 'spring_rolls', 'spaghetti_carbonara',\n",
        "#               'strawberry_shortcake']\n",
        "# dest = 'subset_images'\n",
        "# create_subset_func(pick_label, dest)\n",
        "\n",
        "# Load the label of model from labels.txt\n",
        "with open('./meta/meta/labels.txt') as file:\n",
        "    lines = file.readlines()\n",
        "    labels = [line.rstrip() for line in lines]\n",
        "\n",
        "print(labels)\n",
        "print(\"Total item: \", len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D1KTC0wFwbv",
        "outputId": "61d88120-28f4-4483-9470-25ba3a29c6c6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Apple pie', 'Baby back ribs', 'Baklava', 'Beef carpaccio', 'Beef tartare', 'Beet salad', 'Beignets', 'Bibimbap', 'Bread pudding', 'Breakfast burrito', 'Bruschetta', 'Caesar salad', 'Cannoli', 'Caprese salad', 'Carrot cake', 'Ceviche', 'Cheesecake', 'Cheese plate', 'Chicken curry', 'Chicken quesadilla', 'Chicken wings', 'Chocolate cake', 'Chocolate mousse', 'Churros', 'Clam chowder', 'Club sandwich', 'Crab cakes', 'Creme brulee', 'Croque madame', 'Cup cakes', 'Deviled eggs', 'Donuts', 'Dumplings', 'Edamame', 'Eggs benedict', 'Escargots', 'Falafel', 'Filet mignon', 'Fish and chips', 'Foie gras', 'French fries', 'French onion soup', 'French toast', 'Fried calamari', 'Fried rice', 'Frozen yogurt', 'Garlic bread', 'Gnocchi', 'Greek salad', 'Grilled cheese sandwich', 'Grilled salmon', 'Guacamole', 'Gyoza', 'Hamburger', 'Hot and sour soup', 'Hot dog', 'Huevos rancheros', 'Hummus', 'Ice cream', 'Lasagna', 'Lobster bisque', 'Lobster roll sandwich', 'Macaroni and cheese', 'Macarons', 'Miso soup', 'Mussels', 'Nachos', 'Omelette', 'Onion rings', 'Oysters', 'Pad thai', 'Paella', 'Pancakes', 'Panna cotta', 'Peking duck', 'Pho', 'Pizza', 'Pork chop', 'Poutine', 'Prime rib', 'Pulled pork sandwich', 'Ramen', 'Ravioli', 'Red velvet cake', 'Risotto', 'Samosa', 'Sashimi', 'Scallops', 'Seaweed salad', 'Shrimp and grits', 'Spaghetti bolognese', 'Spaghetti carbonara', 'Spring rolls', 'Steak', 'Strawberry shortcake', 'Sushi', 'Tacos', 'Takoyaki', 'Tiramisu', 'Tuna tartare', 'Waffles']\n",
            "Total item:  101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "IMG_WIDTH, IMG_HEIGHT = 299, 299\n",
        "\n",
        "def data_generator_func(data_dir, img_width, img_height, batch_size):\n",
        "    train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                      rotation_range=60,\n",
        "                                      width_shift_range=0.2,\n",
        "                                      height_shift_range=0.2,\n",
        "                                      shear_range=0.2,\n",
        "                                      zoom_range=0.2,\n",
        "                                      horizontal_flip=True,\n",
        "                                      fill_mode='nearest',\n",
        "                                      validation_split=0.2)\n",
        "    \n",
        "    train_generator = train_datagen.flow_from_directory(directory=data_dir,\n",
        "                                                        target_size=(img_width, img_height),\n",
        "                                                        batch_size=batch_size,\n",
        "                                                        shuffle=True,\n",
        "                                                        class_mode='categorical',\n",
        "                                                        subset='training')\n",
        "    \n",
        "    val_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                    validation_split=0.2)\n",
        "    \n",
        "    val_generator = val_datagen.flow_from_directory(directory=data_dir,\n",
        "                                                    target_size=(img_width, img_height),\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    subset='validation')\n",
        "    \n",
        "    return train_generator, val_generator\n",
        "\n",
        "#train_generator, val_generator = data_generator_func('subset_images', IMG_WIDTH, IMG_HEIGHT, BATCH_SIZE)\n",
        "#train_generator, val_generator = data_generator_func(DATA_DIR, IMG_WIDTH, IMG_HEIGHT, BATCH_SIZE)\n",
        "train_generator, val_generator = data_generator_func('images', IMG_WIDTH, IMG_HEIGHT, BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "UjXeImAul1_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e232b19-99e1-4000-81cc-51ffce888974"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 80800 images belonging to 101 classes.\n",
            "Found 20200 images belonging to 101 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_generator = 80800\n",
        "num_val_generator = 20200"
      ],
      "metadata": {
        "id": "92TeDF7gl2D-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_dict = train_generator.class_indices\n",
        "labels_dict"
      ],
      "metadata": {
        "id": "2eG9IAW3l2Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continuation of Last Model"
      ],
      "metadata": {
        "id": "7K2z8rMvmE3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes of learning rate\n",
        "\n",
        "One of hyperparameter to tune.\n",
        "> The learning rate is perhaps the most important hyperparameter. If you have time to tune only one hyperparameter, tune the learning rate.\n",
        " - Page 429, Deep Leanring, 2016\n",
        "\n",
        "Unfortunately, discovering good learning rate only through trial-and-error.\n",
        "\n",
        "---\n",
        "Learning rate interact with other aspect of optimization processs, and interaction may be nonlinear. Generally, smaller learning rate require more training epochs. Further, **smaller batch size better suited for smaller learning rate**, given the noisy estimate of the error gradient."
      ],
      "metadata": {
        "id": "EwCgfnH9m0ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "input_shape = 299\n",
        "icv3 = InceptionV3(input_shape=(input_shape, input_shape, 3),\n",
        "                   weights='imagenet',\n",
        "                   include_top=False)\n",
        "\n",
        "for layer in icv3.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "fkxYa2ugl2HX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes on Batch Size, Step per epochs, and Validation steps\n",
        "\n",
        "Sc: https://datascience.stackexchange.com/questions/29719/how-to-set-batch-size-steps-per-epoch-and-validation-steps\n",
        "\n",
        "- batch_size, determine number of samples in each mini batch. \n",
        "  - Maximum is all samples, result in gradient descent accurate with loss into minimum (if learning rate small enough), but iterations are slower.\n",
        "  - Minimum is 1, result in stochastic gradient descent: fast but the direction of gradient step based on only 1 sample. \n",
        "\n",
        "- steps_per_epoch, number of batch iterations before training epochs considered finished.\n",
        "\n",
        "- validation_steps, same as steps_per_epoch but for validation set."
      ],
      "metadata": {
        "id": "aodqNSvSokpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(output_units):\n",
        "  model = tf.keras.Sequential([\n",
        "      icv3,\n",
        "      tf.keras.layers.MaxPooling2D(),\n",
        "      tf.keras.layers.Dense(512, activation='relu'),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(output_units, kernel_regularizer=tf.keras.regularizers.l2(5e-3), activation='softmax')\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  # Creating callbacks\n",
        "  checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='model/checkpoint/checkpoint_model.hdf5',\n",
        "                                                          save_best_only=True,\n",
        "                                                          verbose=1)\n",
        "\n",
        "  csvlogger_callback = tf.keras.callbacks.CSVLogger('model/csv_log/csvlog_model.log')\n",
        "\n",
        "  # Fit the model\n",
        "  history = model.fit(\n",
        "      train_generator,\n",
        "      validation_data=val_generator,\n",
        "      batch_size=128,\n",
        "      steps_per_epoch=num_train_generator // BATCH_SIZE,\n",
        "      validation_steps=num_val_generator // BATCH_SIZE,\n",
        "      epochs=30,\n",
        "      callbacks=[csvlogger_callback, checkpoint_callback],\n",
        "      verbose=1\n",
        "  )\n",
        "\n",
        "  return model, history"
      ],
      "metadata": {
        "id": "VdLDGXrjV0qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cont_model_icv3 = tf.keras.Sequential([\n",
        "#     icv3,\n",
        "#     tf.keras.layers.MaxPooling2D(),\n",
        "#     tf.keras.layers.Dense(512, activation='relu'),\n",
        "#     tf.keras.layers.Dropout(0.2),\n",
        "#     tf.keras.layers.Flatten(),\n",
        "#     tf.keras.layers.Dense(3, kernel_regularizer=tf.keras.regularizers.l2(5e-3), activation='softmax')\n",
        "# ])\n",
        "\n",
        "# fifth_model_icv3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "#                           loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "#                           metrics=['accuracy'])\n",
        "\n",
        "# # Creating callbacks\n",
        "# checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='model/checkpoint/checkpoint5th_3class_adam.hdf5',\n",
        "#                                                          save_best_only=True,\n",
        "#                                                          verbose=1)\n",
        "\n",
        "# csvlogger_callback = tf.keras.callbacks.CSVLogger('model/csv_log/csvlog5th_3class_adam.log')\n",
        "\n",
        "# # Fit the model\n",
        "# history_fifth_model_icv3 = fifth_model_icv3.fit(\n",
        "#     train_generator,\n",
        "#     validation_data=val_generator,\n",
        "#     batch_size=128,\n",
        "#     steps_per_epoch=num_train_generator // BATCH_SIZE,\n",
        "#     validation_steps=num_val_generator // BATCH_SIZE,\n",
        "#     epochs=30,\n",
        "#     callbacks=[csvlogger_callback, checkpoint_callback],\n",
        "#     verbose=1\n",
        "# )\n",
        "\n",
        "# fifth_model_icv3.save('model/model5th_icv3_adam.hdf5')"
      ],
      "metadata": {
        "id": "UshTKmaxl2Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cont_model, cont_history = train_model(11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tIA8aOwXE9c",
        "outputId": "b99963fa-c81c-4d17-c8ec-aab111fd4a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 1.7284 - accuracy: 0.4910\n",
            "Epoch 1: val_loss improved from inf to 0.93304, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 173s 2s/step - loss: 1.7284 - accuracy: 0.4910 - val_loss: 0.9330 - val_accuracy: 0.7233\n",
            "Epoch 2/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 1.0504 - accuracy: 0.6960\n",
            "Epoch 2: val_loss improved from 0.93304 to 0.81535, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 1.0504 - accuracy: 0.6960 - val_loss: 0.8154 - val_accuracy: 0.7679\n",
            "Epoch 3/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.9297 - accuracy: 0.7294\n",
            "Epoch 3: val_loss improved from 0.81535 to 0.74579, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.9297 - accuracy: 0.7294 - val_loss: 0.7458 - val_accuracy: 0.7946\n",
            "Epoch 4/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.8598 - accuracy: 0.7542\n",
            "Epoch 4: val_loss improved from 0.74579 to 0.70037, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.8598 - accuracy: 0.7542 - val_loss: 0.7004 - val_accuracy: 0.8070\n",
            "Epoch 5/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.8296 - accuracy: 0.7616\n",
            "Epoch 5: val_loss improved from 0.70037 to 0.68633, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.8296 - accuracy: 0.7616 - val_loss: 0.6863 - val_accuracy: 0.8093\n",
            "Epoch 6/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.7709 - accuracy: 0.7798\n",
            "Epoch 6: val_loss improved from 0.68633 to 0.66175, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.7709 - accuracy: 0.7798 - val_loss: 0.6618 - val_accuracy: 0.8153\n",
            "Epoch 7/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.7575 - accuracy: 0.7833\n",
            "Epoch 7: val_loss improved from 0.66175 to 0.63498, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.7575 - accuracy: 0.7833 - val_loss: 0.6350 - val_accuracy: 0.8240\n",
            "Epoch 8/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.7250 - accuracy: 0.7942\n",
            "Epoch 8: val_loss did not improve from 0.63498\n",
            "68/68 [==============================] - 152s 2s/step - loss: 0.7250 - accuracy: 0.7942 - val_loss: 0.6395 - val_accuracy: 0.8254\n",
            "Epoch 9/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.7109 - accuracy: 0.7980\n",
            "Epoch 9: val_loss improved from 0.63498 to 0.61993, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.7109 - accuracy: 0.7980 - val_loss: 0.6199 - val_accuracy: 0.8309\n",
            "Epoch 10/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.6908 - accuracy: 0.7995\n",
            "Epoch 10: val_loss improved from 0.61993 to 0.60261, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 155s 2s/step - loss: 0.6908 - accuracy: 0.7995 - val_loss: 0.6026 - val_accuracy: 0.8359\n",
            "Epoch 11/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.6775 - accuracy: 0.8080\n",
            "Epoch 11: val_loss improved from 0.60261 to 0.59073, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 154s 2s/step - loss: 0.6775 - accuracy: 0.8080 - val_loss: 0.5907 - val_accuracy: 0.8410\n",
            "Epoch 12/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.6424 - accuracy: 0.8211\n",
            "Epoch 12: val_loss improved from 0.59073 to 0.58243, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 154s 2s/step - loss: 0.6424 - accuracy: 0.8211 - val_loss: 0.5824 - val_accuracy: 0.8405\n",
            "Epoch 13/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.6496 - accuracy: 0.8145\n",
            "Epoch 13: val_loss did not improve from 0.58243\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.6496 - accuracy: 0.8145 - val_loss: 0.5866 - val_accuracy: 0.8373\n",
            "Epoch 14/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.6279 - accuracy: 0.8209\n",
            "Epoch 14: val_loss improved from 0.58243 to 0.56796, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.6279 - accuracy: 0.8209 - val_loss: 0.5680 - val_accuracy: 0.8382\n",
            "Epoch 15/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.6166 - accuracy: 0.8269\n",
            "Epoch 15: val_loss did not improve from 0.56796\n",
            "68/68 [==============================] - 154s 2s/step - loss: 0.6166 - accuracy: 0.8269 - val_loss: 0.5750 - val_accuracy: 0.8378\n",
            "Epoch 16/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.6112 - accuracy: 0.8265\n",
            "Epoch 16: val_loss improved from 0.56796 to 0.56394, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.6112 - accuracy: 0.8265 - val_loss: 0.5639 - val_accuracy: 0.8433\n",
            "Epoch 17/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5842 - accuracy: 0.8350\n",
            "Epoch 17: val_loss improved from 0.56394 to 0.56196, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 152s 2s/step - loss: 0.5842 - accuracy: 0.8350 - val_loss: 0.5620 - val_accuracy: 0.8405\n",
            "Epoch 18/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5816 - accuracy: 0.8341\n",
            "Epoch 18: val_loss improved from 0.56196 to 0.55172, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 152s 2s/step - loss: 0.5816 - accuracy: 0.8341 - val_loss: 0.5517 - val_accuracy: 0.8447\n",
            "Epoch 19/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5700 - accuracy: 0.8365\n",
            "Epoch 19: val_loss improved from 0.55172 to 0.54765, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 152s 2s/step - loss: 0.5700 - accuracy: 0.8365 - val_loss: 0.5476 - val_accuracy: 0.8520\n",
            "Epoch 20/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5644 - accuracy: 0.8404\n",
            "Epoch 20: val_loss improved from 0.54765 to 0.54740, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 152s 2s/step - loss: 0.5644 - accuracy: 0.8404 - val_loss: 0.5474 - val_accuracy: 0.8488\n",
            "Epoch 21/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5559 - accuracy: 0.8421\n",
            "Epoch 21: val_loss improved from 0.54740 to 0.53900, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.5559 - accuracy: 0.8421 - val_loss: 0.5390 - val_accuracy: 0.8483\n",
            "Epoch 22/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5502 - accuracy: 0.8383\n",
            "Epoch 22: val_loss improved from 0.53900 to 0.53157, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 156s 2s/step - loss: 0.5502 - accuracy: 0.8383 - val_loss: 0.5316 - val_accuracy: 0.8511\n",
            "Epoch 23/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5409 - accuracy: 0.8410\n",
            "Epoch 23: val_loss improved from 0.53157 to 0.52669, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.5409 - accuracy: 0.8410 - val_loss: 0.5267 - val_accuracy: 0.8479\n",
            "Epoch 24/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5258 - accuracy: 0.8501\n",
            "Epoch 24: val_loss improved from 0.52669 to 0.51839, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.5258 - accuracy: 0.8501 - val_loss: 0.5184 - val_accuracy: 0.8529\n",
            "Epoch 25/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5201 - accuracy: 0.8506\n",
            "Epoch 25: val_loss did not improve from 0.51839\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.5201 - accuracy: 0.8506 - val_loss: 0.5316 - val_accuracy: 0.8502\n",
            "Epoch 26/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5023 - accuracy: 0.8561\n",
            "Epoch 26: val_loss did not improve from 0.51839\n",
            "68/68 [==============================] - 157s 2s/step - loss: 0.5023 - accuracy: 0.8561 - val_loss: 0.5273 - val_accuracy: 0.8516\n",
            "Epoch 27/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5194 - accuracy: 0.8544\n",
            "Epoch 27: val_loss did not improve from 0.51839\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.5194 - accuracy: 0.8544 - val_loss: 0.5202 - val_accuracy: 0.8516\n",
            "Epoch 28/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5119 - accuracy: 0.8503\n",
            "Epoch 28: val_loss improved from 0.51839 to 0.50856, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.5119 - accuracy: 0.8503 - val_loss: 0.5086 - val_accuracy: 0.8562\n",
            "Epoch 29/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.4942 - accuracy: 0.8600\n",
            "Epoch 29: val_loss did not improve from 0.50856\n",
            "68/68 [==============================] - 152s 2s/step - loss: 0.4942 - accuracy: 0.8600 - val_loss: 0.5221 - val_accuracy: 0.8506\n",
            "Epoch 30/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.4825 - accuracy: 0.8627\n",
            "Epoch 30: val_loss did not improve from 0.50856\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.4825 - accuracy: 0.8627 - val_loss: 0.5099 - val_accuracy: 0.8621\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cont_model.save(\"model/cont_model_1\")\n",
        "\n",
        "!zip -r ./cont_model_1.zip model\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"./cont_model_1.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "k44y20HYXE7X",
        "outputId": "7021e86b-fc0a-4dc3-90cb-4c024218885e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: model/cont_model_1/assets\n",
            "  adding: model/ (stored 0%)\n",
            "  adding: model/checkpoint/ (stored 0%)\n",
            "  adding: model/checkpoint/checkpoint_model.hdf5 (deflated 8%)\n",
            "  adding: model/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: model/csv_log/ (stored 0%)\n",
            "  adding: model/csv_log/csvlog_model.log (deflated 52%)\n",
            "  adding: model/cont_model_1/ (stored 0%)\n",
            "  adding: model/cont_model_1/assets/ (stored 0%)\n",
            "  adding: model/cont_model_1/keras_metadata.pb (deflated 96%)\n",
            "  adding: model/cont_model_1/saved_model.pb (deflated 92%)\n",
            "  adding: model/cont_model_1/variables/ (stored 0%)\n",
            "  adding: model/cont_model_1/variables/variables.index (deflated 76%)\n",
            "  adding: model/cont_model_1/variables/variables.data-00000-of-00001 (deflated 7%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_71bdce03-06bd-41d2-a62b-10a29e6e4ab0\", \"cont_model_1.zip\", 186885184)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training on all classes"
      ],
      "metadata": {
        "id": "2XSkEK7pGTAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing Tensorboards\n",
        "import datetime\n",
        "\n",
        "def tensorboard_callback():\n",
        "  log_dir = \"./drive/MyDrive/Capstone_Related/model_icv3_101class/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  \n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
        "\n",
        "  return tensorboard_callback"
      ],
      "metadata": {
        "id": "USHVbRyuHM7A"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SHAPE = (299, 299, 3)\n",
        "BATCH_SIZE = 128\n",
        "NUM_CLASSES = 101\n",
        "\n",
        "icv3 = tf.keras.applications.InceptionV3(input_shape=INPUT_SHAPE, include_top=False)\n",
        "\n",
        "def create_model(base_model, num_classes=NUM_CLASSES):\n",
        "  # Freezing EfficientNet layer\n",
        "  base_model.trainable = False\n",
        "\n",
        "  x = base_model.output\n",
        "  x = tf.keras.layers.MaxPooling2D()(x)\n",
        "  x = tf.keras.layers.Dense(units=512, activation='relu')(x)\n",
        "  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  x = tf.keras.layers.Flatten()(x)\n",
        "  outputs = tf.keras.layers.Dense(units=num_classes, kernel_regularizer=tf.keras.regularizers.l2(5e-3), activation='softmax')(x)\n",
        "\n",
        "  model = tf.keras.Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "  # Compiling model\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "wJ4aeCVsGVJG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_icv3 = create_model(icv3)\n",
        "model_path = \"./drive/MyDrive/Capstone_Related/model_icv3_101class\"\n",
        "\n",
        "# Creating callbacks\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='./drive/MyDrive/Capstone_Related/model_icv3_101class/checkpoint/checkpoint_icv3.hdf5',\n",
        "                                                          save_best_only=True,\n",
        "                                                          verbose=1)\n",
        "\n",
        "csvlogger_callback = tf.keras.callbacks.CSVLogger('./drive/MyDrive/Capstone_Related/model_icv3_101class/csv_log/csvlog_icv3.log')\n",
        "\n",
        "history_icv3 = model_icv3.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=30,\n",
        "    steps_per_epoch=num_train_generator // BATCH_SIZE,\n",
        "    validation_steps=num_val_generator // BATCH_SIZE,\n",
        "    callbacks=[csvlogger_callback, checkpoint_callback, tensorboard_callback()],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "id": "ZhKzogO2GVDo",
        "outputId": "871e18b8-3e5b-4e3f-a2aa-380fe4a4c7ad"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: ./drive/MyDrive/Capstone_Related/model_icv3_101class/20220605-091022\n",
            "Epoch 1/30\n",
            "631/631 [==============================] - ETA: 0s - loss: 3.8861 - accuracy: 0.2891\n",
            "Epoch 1: val_loss improved from inf to 2.81080, saving model to ./drive/MyDrive/Capstone_Related/model_icv3_101class/checkpoint/checkpoint_icv3.hdf5\n",
            "631/631 [==============================] - 2017s 3s/step - loss: 3.8861 - accuracy: 0.2891 - val_loss: 2.8108 - val_accuracy: 0.4801\n",
            "Epoch 2/30\n",
            "631/631 [==============================] - ETA: 0s - loss: 2.8877 - accuracy: 0.4446\n",
            "Epoch 2: val_loss improved from 2.81080 to 2.44429, saving model to ./drive/MyDrive/Capstone_Related/model_icv3_101class/checkpoint/checkpoint_icv3.hdf5\n",
            "631/631 [==============================] - 1986s 3s/step - loss: 2.8877 - accuracy: 0.4446 - val_loss: 2.4443 - val_accuracy: 0.5324\n",
            "Epoch 3/30\n",
            "631/631 [==============================] - ETA: 0s - loss: 2.5690 - accuracy: 0.4889\n",
            "Epoch 3: val_loss improved from 2.44429 to 2.22251, saving model to ./drive/MyDrive/Capstone_Related/model_icv3_101class/checkpoint/checkpoint_icv3.hdf5\n",
            "631/631 [==============================] - 1942s 3s/step - loss: 2.5690 - accuracy: 0.4889 - val_loss: 2.2225 - val_accuracy: 0.5625\n",
            "Epoch 4/30\n",
            "631/631 [==============================] - ETA: 0s - loss: 2.3601 - accuracy: 0.5176\n",
            "Epoch 4: val_loss improved from 2.22251 to 2.09534, saving model to ./drive/MyDrive/Capstone_Related/model_icv3_101class/checkpoint/checkpoint_icv3.hdf5\n",
            "631/631 [==============================] - 1992s 3s/step - loss: 2.3601 - accuracy: 0.5176 - val_loss: 2.0953 - val_accuracy: 0.5734\n",
            "Epoch 5/30\n",
            "631/631 [==============================] - ETA: 0s - loss: 2.2092 - accuracy: 0.5356\n",
            "Epoch 5: val_loss improved from 2.09534 to 1.98058, saving model to ./drive/MyDrive/Capstone_Related/model_icv3_101class/checkpoint/checkpoint_icv3.hdf5\n",
            "631/631 [==============================] - 1969s 3s/step - loss: 2.2092 - accuracy: 0.5356 - val_loss: 1.9806 - val_accuracy: 0.5868\n",
            "Epoch 6/30\n",
            "631/631 [==============================] - ETA: 0s - loss: 2.0942 - accuracy: 0.5504\n",
            "Epoch 6: val_loss improved from 1.98058 to 1.90411, saving model to ./drive/MyDrive/Capstone_Related/model_icv3_101class/checkpoint/checkpoint_icv3.hdf5\n",
            "631/631 [==============================] - 1954s 3s/step - loss: 2.0942 - accuracy: 0.5504 - val_loss: 1.9041 - val_accuracy: 0.5955\n",
            "Epoch 7/30\n",
            "631/631 [==============================] - ETA: 0s - loss: 2.0047 - accuracy: 0.5615\n",
            "Epoch 7: val_loss improved from 1.90411 to 1.82668, saving model to ./drive/MyDrive/Capstone_Related/model_icv3_101class/checkpoint/checkpoint_icv3.hdf5\n",
            "631/631 [==============================] - 1971s 3s/step - loss: 2.0047 - accuracy: 0.5615 - val_loss: 1.8267 - val_accuracy: 0.6030\n",
            "Epoch 8/30\n",
            "170/631 [=======>......................] - ETA: 21:59 - loss: 1.9495 - accuracy: 0.5696"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-6112112370d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_val_generator\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcsvlogger_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('./history_icv3', 'wb') as file_pi:\n",
        "  pickle.dump(history_icv3.history, file_pi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "RWnqW6Ip_l9p",
        "outputId": "705eba03-cab3-4047-d27b-c9bf374f4043"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-f648ddfeb5ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./history_icv3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_pi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_icv3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_pi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'history_icv3' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WUR1HjxNXE5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First EfficientNetB2 Model"
      ],
      "metadata": {
        "id": "XUUgB9O93wRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute this when loading model from previous checkpoint\n",
        "model_path = \"./model/checkpoint/checkpoint_model.hdf5\"\n",
        "\n",
        "model_efnetb2_1 = tf.keras.models.load_model(model_path)"
      ],
      "metadata": {
        "id": "ndTHXkaYdQ46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EfficientNet doesn't use rescale as its already in their preprocessing layer\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "# IMG_WIDTH, IMG_HEIGHT = 224, 224  # For EfficientNetB0\n",
        "IMG_WIDTH, IMG_HEIGHT = 260, 260  # For EfficientNetB2\n",
        "\n",
        "def data_generator_func(data_dir, img_width, img_height, batch_size):\n",
        "    train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                      rotation_range=60,\n",
        "                                      width_shift_range=0.2,\n",
        "                                      height_shift_range=0.2,\n",
        "                                      shear_range=0.2,\n",
        "                                      zoom_range=0.2,\n",
        "                                      horizontal_flip=True,\n",
        "                                      fill_mode='nearest',\n",
        "                                      validation_split=0.2)\n",
        "    \n",
        "    train_generator = train_datagen.flow_from_directory(directory=data_dir,\n",
        "                                                        target_size=(img_width, img_height),\n",
        "                                                        batch_size=batch_size,\n",
        "                                                        shuffle=True,\n",
        "                                                        class_mode='categorical',\n",
        "                                                        subset='training')\n",
        "    \n",
        "    val_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                    validation_split=0.2)\n",
        "    \n",
        "    val_generator = val_datagen.flow_from_directory(directory=data_dir,\n",
        "                                                    target_size=(img_width, img_height),\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    subset='validation')\n",
        "    \n",
        "    return train_generator, val_generator\n",
        "\n",
        "#train_generator, val_generator = data_generator_func('subset_images', IMG_WIDTH, IMG_HEIGHT, BATCH_SIZE)\n",
        "#train_generator, val_generator = data_generator_func(DATA_DIR, IMG_WIDTH, IMG_HEIGHT, BATCH_SIZE)\n",
        "train_generator, val_generator = data_generator_func('images', IMG_WIDTH, IMG_HEIGHT, BATCH_SIZE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dO3eTjU6wOQ",
        "outputId": "09b1e2f7-0ca9-415a-b42d-44224b88c926"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 80800 images belonging to 101 classes.\n",
            "Found 20200 images belonging to 101 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from tensorflow.keras.applications import EfficientNetB2"
      ],
      "metadata": {
        "id": "FO98OyID6XpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SHAPE = (260, 260, 3)\n",
        "BATCH_SIZE = 128\n",
        "efnetb2 = tf.keras.applications.EfficientNetB2(input_shape=INPUT_SHAPE, include_top=False)\n",
        "\n",
        "def create_model(base_model, num_classes=11):\n",
        "  # Freezing EfficientNet layer\n",
        "  base_model.trainable = False\n",
        "\n",
        "  x = base_model.output\n",
        "  x = tf.keras.layers.MaxPooling2D()(x)\n",
        "  x = tf.keras.layers.Dense(units=512, activation='relu')(x)\n",
        "  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  x = tf.keras.layers.Flatten()(x)\n",
        "  outputs = tf.keras.layers.Dense(units=num_classes, kernel_regularizer=tf.keras.regularizers.l2(5e-3), activation='softmax')(x)\n",
        "\n",
        "  model = tf.keras.Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "  # Compiling model\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "9SCu9eVtXE3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_efnetb2_1 = create_model(efnetb2)\n",
        "\n",
        "# Creating callbacks\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='model/checkpoint/checkpoint_model.hdf5',\n",
        "                                                          save_best_only=True,\n",
        "                                                          verbose=1)\n",
        "\n",
        "csvlogger_callback = tf.keras.callbacks.CSVLogger('model/csv_log/csvlog_model.log')\n",
        "\n",
        "history_efnetb2_1 = model_efnetb2_1.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=30,\n",
        "    steps_per_epoch=num_train_generator // BATCH_SIZE,\n",
        "    validation_steps=num_val_generator // BATCH_SIZE,\n",
        "    callbacks=[csvlogger_callback, checkpoint_callback, tensorboard_callback(\"efnet_ver1\")],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_WU9MUxl2Lj",
        "outputId": "115e8a48-275e-4a98-e4c2-d509cdd25b5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: ./model/tfboardefnet_ver1/20220531-095348\n",
            "Epoch 1/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 1.6151 - accuracy: 0.5180\n",
            "Epoch 1: val_loss improved from inf to 0.76246, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 185s 3s/step - loss: 1.6151 - accuracy: 0.5180 - val_loss: 0.7625 - val_accuracy: 0.7941\n",
            "Epoch 2/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.8646 - accuracy: 0.7561\n",
            "Epoch 2: val_loss improved from 0.76246 to 0.60942, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 177s 3s/step - loss: 0.8646 - accuracy: 0.7561 - val_loss: 0.6094 - val_accuracy: 0.8410\n",
            "Epoch 3/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.7327 - accuracy: 0.7943\n",
            "Epoch 3: val_loss improved from 0.60942 to 0.54667, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 168s 2s/step - loss: 0.7327 - accuracy: 0.7943 - val_loss: 0.5467 - val_accuracy: 0.8603\n",
            "Epoch 4/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.6785 - accuracy: 0.8124\n",
            "Epoch 4: val_loss improved from 0.54667 to 0.50503, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 169s 2s/step - loss: 0.6785 - accuracy: 0.8124 - val_loss: 0.5050 - val_accuracy: 0.8755\n",
            "Epoch 5/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.6299 - accuracy: 0.8299\n",
            "Epoch 5: val_loss improved from 0.50503 to 0.48556, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 175s 3s/step - loss: 0.6299 - accuracy: 0.8299 - val_loss: 0.4856 - val_accuracy: 0.8824\n",
            "Epoch 6/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5947 - accuracy: 0.8401\n",
            "Epoch 6: val_loss improved from 0.48556 to 0.47347, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 175s 3s/step - loss: 0.5947 - accuracy: 0.8401 - val_loss: 0.4735 - val_accuracy: 0.8883\n",
            "Epoch 7/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5687 - accuracy: 0.8514\n",
            "Epoch 7: val_loss improved from 0.47347 to 0.45748, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 167s 2s/step - loss: 0.5687 - accuracy: 0.8514 - val_loss: 0.4575 - val_accuracy: 0.8934\n",
            "Epoch 8/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5443 - accuracy: 0.8559\n",
            "Epoch 8: val_loss improved from 0.45748 to 0.45309, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 167s 2s/step - loss: 0.5443 - accuracy: 0.8559 - val_loss: 0.4531 - val_accuracy: 0.8892\n",
            "Epoch 9/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5113 - accuracy: 0.8691\n",
            "Epoch 9: val_loss improved from 0.45309 to 0.43842, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 175s 3s/step - loss: 0.5113 - accuracy: 0.8691 - val_loss: 0.4384 - val_accuracy: 0.8934\n",
            "Epoch 10/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5085 - accuracy: 0.8647\n",
            "Epoch 10: val_loss improved from 0.43842 to 0.42108, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 174s 3s/step - loss: 0.5085 - accuracy: 0.8647 - val_loss: 0.4211 - val_accuracy: 0.8957\n",
            "Epoch 11/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.4890 - accuracy: 0.8730\n",
            "Epoch 11: val_loss did not improve from 0.42108\n",
            "68/68 [==============================] - 166s 2s/step - loss: 0.4890 - accuracy: 0.8730 - val_loss: 0.4234 - val_accuracy: 0.8957\n",
            "Epoch 12/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.4719 - accuracy: 0.8803\n",
            "Epoch 12: val_loss improved from 0.42108 to 0.42033, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 166s 2s/step - loss: 0.4719 - accuracy: 0.8803 - val_loss: 0.4203 - val_accuracy: 0.9026\n",
            "Epoch 13/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.4614 - accuracy: 0.8824\n",
            "Epoch 13: val_loss improved from 0.42033 to 0.41530, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 167s 2s/step - loss: 0.4614 - accuracy: 0.8824 - val_loss: 0.4153 - val_accuracy: 0.8998\n",
            "Epoch 14/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.4425 - accuracy: 0.8809\n",
            "Epoch 14: val_loss improved from 0.41530 to 0.41246, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 175s 3s/step - loss: 0.4425 - accuracy: 0.8809 - val_loss: 0.4125 - val_accuracy: 0.9017\n",
            "Epoch 15/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.4262 - accuracy: 0.8866\n",
            "Epoch 15: val_loss improved from 0.41246 to 0.40596, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 173s 3s/step - loss: 0.4262 - accuracy: 0.8866 - val_loss: 0.4060 - val_accuracy: 0.9030\n",
            "Epoch 16/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.4150 - accuracy: 0.8902\n",
            "Epoch 16: val_loss improved from 0.40596 to 0.40291, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 173s 3s/step - loss: 0.4150 - accuracy: 0.8902 - val_loss: 0.4029 - val_accuracy: 0.9003\n",
            "Epoch 17/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.4151 - accuracy: 0.8954\n",
            "Epoch 17: val_loss improved from 0.40291 to 0.39611, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 166s 2s/step - loss: 0.4151 - accuracy: 0.8954 - val_loss: 0.3961 - val_accuracy: 0.9021\n",
            "Epoch 18/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3986 - accuracy: 0.8997\n",
            "Epoch 18: val_loss improved from 0.39611 to 0.39445, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 167s 2s/step - loss: 0.3986 - accuracy: 0.8997 - val_loss: 0.3945 - val_accuracy: 0.9030\n",
            "Epoch 19/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3926 - accuracy: 0.8990\n",
            "Epoch 19: val_loss did not improve from 0.39445\n",
            "68/68 [==============================] - 172s 3s/step - loss: 0.3926 - accuracy: 0.8990 - val_loss: 0.3957 - val_accuracy: 0.9030\n",
            "Epoch 20/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3820 - accuracy: 0.9019\n",
            "Epoch 20: val_loss improved from 0.39445 to 0.39401, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 173s 3s/step - loss: 0.3820 - accuracy: 0.9019 - val_loss: 0.3940 - val_accuracy: 0.9007\n",
            "Epoch 21/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3757 - accuracy: 0.9065\n",
            "Epoch 21: val_loss improved from 0.39401 to 0.38515, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 172s 3s/step - loss: 0.3757 - accuracy: 0.9065 - val_loss: 0.3852 - val_accuracy: 0.9072\n",
            "Epoch 22/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3730 - accuracy: 0.9060\n",
            "Epoch 22: val_loss did not improve from 0.38515\n",
            "68/68 [==============================] - 168s 2s/step - loss: 0.3730 - accuracy: 0.9060 - val_loss: 0.3897 - val_accuracy: 0.9044\n",
            "Epoch 23/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3522 - accuracy: 0.9110\n",
            "Epoch 23: val_loss did not improve from 0.38515\n",
            "68/68 [==============================] - 165s 2s/step - loss: 0.3522 - accuracy: 0.9110 - val_loss: 0.3887 - val_accuracy: 0.9017\n",
            "Epoch 24/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3556 - accuracy: 0.9114\n",
            "Epoch 24: val_loss improved from 0.38515 to 0.38135, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 165s 2s/step - loss: 0.3556 - accuracy: 0.9114 - val_loss: 0.3813 - val_accuracy: 0.9085\n",
            "Epoch 25/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3530 - accuracy: 0.9111\n",
            "Epoch 25: val_loss improved from 0.38135 to 0.38018, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 165s 2s/step - loss: 0.3530 - accuracy: 0.9111 - val_loss: 0.3802 - val_accuracy: 0.9076\n",
            "Epoch 26/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3341 - accuracy: 0.9158\n",
            "Epoch 26: val_loss improved from 0.38018 to 0.37834, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 173s 3s/step - loss: 0.3341 - accuracy: 0.9158 - val_loss: 0.3783 - val_accuracy: 0.9076\n",
            "Epoch 27/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3409 - accuracy: 0.9131\n",
            "Epoch 27: val_loss did not improve from 0.37834\n",
            "68/68 [==============================] - 165s 2s/step - loss: 0.3409 - accuracy: 0.9131 - val_loss: 0.3795 - val_accuracy: 0.9072\n",
            "Epoch 28/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3238 - accuracy: 0.9180\n",
            "Epoch 28: val_loss improved from 0.37834 to 0.37140, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 165s 2s/step - loss: 0.3238 - accuracy: 0.9180 - val_loss: 0.3714 - val_accuracy: 0.9090\n",
            "Epoch 29/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3169 - accuracy: 0.9232\n",
            "Epoch 29: val_loss improved from 0.37140 to 0.36618, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 165s 2s/step - loss: 0.3169 - accuracy: 0.9232 - val_loss: 0.3662 - val_accuracy: 0.9104\n",
            "Epoch 30/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3220 - accuracy: 0.9181\n",
            "Epoch 30: val_loss did not improve from 0.36618\n",
            "68/68 [==============================] - 166s 2s/step - loss: 0.3220 - accuracy: 0.9181 - val_loss: 0.3758 - val_accuracy: 0.9095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qFFsTi-oe036"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r ./model_efnetb2_1.zip model\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"./model_efnetb2_1.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "Sel1d0Ax-p8l",
        "outputId": "117ca38e-820f-415e-c4e2-e30251bd9883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: model/ (stored 0%)\n",
            "  adding: model/checkpoint/ (stored 0%)\n",
            "  adding: model/checkpoint/checkpoint_model.hdf5 (deflated 10%)\n",
            "  adding: model/checkpoint/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: model/tfboardefnet_ver1/ (stored 0%)\n",
            "  adding: model/tfboardefnet_ver1/20220531-095348/ (stored 0%)\n",
            "  adding: model/tfboardefnet_ver1/20220531-095348/validation/ (stored 0%)\n",
            "  adding: model/tfboardefnet_ver1/20220531-095348/validation/events.out.tfevents.1653990991.92391deb65b5.75.2.v2 (deflated 77%)\n",
            "  adding: model/tfboardefnet_ver1/20220531-095348/train/ (stored 0%)\n",
            "  adding: model/tfboardefnet_ver1/20220531-095348/train/events.out.tfevents.1653990830.92391deb65b5.75.1.v2 (deflated 93%)\n",
            "  adding: model/tfboardefnet_ver1/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: model/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: model/csv_log/ (stored 0%)\n",
            "  adding: model/csv_log/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: model/csv_log/csvlog_model.log (deflated 53%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b7aaf312-f304-4510-bcac-6d6bbb685bf9\", \"model_efnetb2_1.zip\", 37813041)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Rz3eEAlA-p6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TensorBoard Visualize and Playground"
      ],
      "metadata": {
        "id": "a4KJzSvF4OXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "def tensorboard_callback(experiment_name):\n",
        "  log_dir = \"./model/tfboard\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  \n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
        "\n",
        "  return tensorboard_callback"
      ],
      "metadata": {
        "id": "ROXEfJ-b4R_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator.samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0_FOpG14TWK",
        "outputId": "5318ae5d-9b9d-4974-a526-43320a7820f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8800"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip files\n",
        "!unzip model_efnet_1.zip"
      ],
      "metadata": {
        "id": "bIvAaRhm4TUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50228d81-3736-453a-a727-97331e328ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  model_efnet_1.zip\n",
            "   creating: model/\n",
            "   creating: model/checkpoint/\n",
            "  inflating: model/checkpoint/checkpoint_model.hdf5  \n",
            "   creating: model/tfboardefnet_ver1/\n",
            "   creating: model/tfboardefnet_ver1/20220529-132959/\n",
            "   creating: model/tfboardefnet_ver1/20220529-132959/train/\n",
            "  inflating: model/tfboardefnet_ver1/20220529-132959/train/events.out.tfevents.1653831002.3fcddb13b51f.85.0.v2  \n",
            "   creating: model/tfboardefnet_ver1/20220529-133135/\n",
            "   creating: model/tfboardefnet_ver1/20220529-133135/validation/\n",
            "  inflating: model/tfboardefnet_ver1/20220529-133135/validation/events.out.tfevents.1653831273.3fcddb13b51f.85.2.v2  \n",
            "   creating: model/tfboardefnet_ver1/20220529-133135/train/\n",
            "  inflating: model/tfboardefnet_ver1/20220529-133135/train/events.out.tfevents.1653831097.3fcddb13b51f.85.1.v2  \n",
            "   creating: model/.ipynb_checkpoints/\n",
            "   creating: model/csv_log/\n",
            "  inflating: model/csv_log/csvlog_model.log  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6SudIDDv4TSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uCPW5KFl4TQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1ZVM9-lc4TOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Q3BXke7f4TMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VAG7lPuV4TJq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}