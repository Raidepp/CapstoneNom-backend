{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProjectNom - Iter3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1MGj2WwexWTlp_mort6-K9bG8JivaT3ar",
      "authorship_tag": "ABX9TyM4+Jy9ZZ8aN6Gz/auhgVp4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raidepp/CapstoneNom-backend/blob/main/ProjectNom_Iter3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VbWwixvJlvCF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r ~/.kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp ./kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "1hG2LUIll16L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d kmader/food41\n",
        "!unzip food41.zip"
      ],
      "metadata": {
        "id": "GOQPRndxl18A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_h5 = 'food_c101_n10099_r64x64x3.h5'\n",
        "test_h5 = 'food_test_c101_n1000_r64x64x3.h5'\n",
        "\n",
        "fl = h5py.File(test_h5, 'r')\n",
        "test_img = fl['images']\n",
        "\n",
        "#print(test_img[0])"
      ],
      "metadata": {
        "id": "zyQGffMdl19n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Sub"
      ],
      "metadata": {
        "id": "3wF0kHdql_3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from shutil import copytree, rmtree\n",
        "\n",
        "# Try using few labels\n",
        "def create_subset_func(ls, dest):\n",
        "  if os.path.exists(dest):\n",
        "    rmtree(dest)\n",
        "  os.makedirs(dest)\n",
        "\n",
        "  for item in ls:\n",
        "    # Copying all images in picked list into new directory\n",
        "    copytree( os.path.join('images', item), os.path.join(dest, item) )\n",
        "\n",
        "# pick_label = ['donuts', 'carrot_cake', 'tiramisu', 'fried_rice', 'french_fries', 'omelette',\n",
        "#               'takoyaki', 'hot_dog', 'creme_brulee', 'chocolate_mousse', 'ramen', ]\n",
        "\n",
        "# pick_label = ['donuts', 'tiramisu', 'creme_brulee', 'chocolate_mousse' ]  # 'carrot_cake', \n",
        "# pick_label = ['pizza', 'omelette', 'apple_pie']\n",
        "# pick_label = ['apple_pie', 'beef_carpaccio', 'bibimbap', 'cup_cakes', 'foie_gras', \n",
        "#               'french_fries', 'garlic_bread', 'pizza', 'spring_rolls', 'spaghetti_carbonara',\n",
        "#               'strawberry_shortcake']\n",
        "# dest = 'subset_images'\n",
        "# create_subset_func(pick_label, dest)\n",
        "\n",
        "# Load the label of model from labels.txt\n",
        "with open('./meta/meta/labels.txt') as file:\n",
        "    lines = file.readlines()\n",
        "    labels = [line.rstrip() for line in lines]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UjXeImAul1_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMG WIDTH AND HEIGHT of (256x256) for standard size\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "# IMG_WIDTH, IMG_HEIGHT = 256, 256  # For InceptionV3\n",
        "# IMG_WIDTH, IMG_HEIGHT = 224, 224  # For EfficientNetB0\n",
        "IMG_WIDTH, IMG_HEIGHT = 260, 260  # For EfficientNetB2\n",
        "\n",
        "def data_generator_func(data_dir, img_width, img_height, batch_size):\n",
        "    train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                      rotation_range=60,\n",
        "                                      width_shift_range=0.2,\n",
        "                                      height_shift_range=0.2,\n",
        "                                      shear_range=0.2,\n",
        "                                      zoom_range=0.2,\n",
        "                                      horizontal_flip=True,\n",
        "                                      fill_mode='nearest',\n",
        "                                      validation_split=0.2)\n",
        "    \n",
        "    train_generator = train_datagen.flow_from_directory(directory=data_dir,\n",
        "                                                        target_size=(img_width, img_height),\n",
        "                                                        batch_size=batch_size,\n",
        "                                                        shuffle=True,\n",
        "                                                        class_mode='categorical',\n",
        "                                                        subset='training')\n",
        "    \n",
        "    val_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                    validation_split=0.2)\n",
        "    \n",
        "    val_generator = val_datagen.flow_from_directory(directory=data_dir,\n",
        "                                                    target_size=(img_width, img_height),\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    subset='validation')\n",
        "    \n",
        "    return train_generator, val_generator\n",
        "\n",
        "train_generator, val_generator = data_generator_func('subset_images', IMG_WIDTH, IMG_HEIGHT, BATCH_SIZE)\n",
        "#train_generator, val_generator = data_generator_func(DATA_DIR, IMG_WIDTH, IMG_HEIGHT, BATCH_SIZE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFU30d6al2B3",
        "outputId": "98c6869e-2a77-4f9c-9f01-678634c871a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8800 images belonging to 11 classes.\n",
            "Found 2200 images belonging to 11 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_generator = 8800\n",
        "num_val_generator = 2200"
      ],
      "metadata": {
        "id": "92TeDF7gl2D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2eG9IAW3l2Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continuation of Last Model"
      ],
      "metadata": {
        "id": "7K2z8rMvmE3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes of learning rate\n",
        "\n",
        "One of hyperparameter to tune.\n",
        "> The learning rate is perhaps the most important hyperparameter. If you have time to tune only one hyperparameter, tune the learning rate.\n",
        " - Page 429, Deep Leanring, 2016\n",
        "\n",
        "Unfortunately, discovering good learning rate only through trial-and-error.\n",
        "\n",
        "---\n",
        "Learning rate interact with other aspect of optimization processs, and interaction may be nonlinear. Generally, smaller learning rate require more training epochs. Further, **smaller batch size better suited for smaller learning rate**, given the noisy estimate of the error gradient."
      ],
      "metadata": {
        "id": "EwCgfnH9m0ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "icv3 = InceptionV3(input_shape=(256, 256, 3),\n",
        "                   weights='imagenet',\n",
        "                   include_top=False)\n",
        "\n",
        "for layer in icv3.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkxYa2ugl2HX",
        "outputId": "ee0146db-67a7-4eeb-aae9-fb4f8d327dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 0s 0us/step\n",
            "87924736/87910968 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes on Batch Size, Step per epochs, and Validation steps\n",
        "\n",
        "Sc: https://datascience.stackexchange.com/questions/29719/how-to-set-batch-size-steps-per-epoch-and-validation-steps\n",
        "\n",
        "- batch_size, determine number of samples in each mini batch. \n",
        "  - Maximum is all samples, result in gradient descent accurate with loss into minimum (if learning rate small enough), but iterations are slower.\n",
        "  - Minimum is 1, result in stochastic gradient descent: fast but the direction of gradient step based on only 1 sample. \n",
        "\n",
        "- steps_per_epoch, number of batch iterations before training epochs considered finished.\n",
        "\n",
        "- validation_steps, same as steps_per_epoch but for validation set."
      ],
      "metadata": {
        "id": "aodqNSvSokpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(output_units):\n",
        "  model = tf.keras.Sequential([\n",
        "      icv3,\n",
        "      tf.keras.layers.MaxPooling2D(),\n",
        "      tf.keras.layers.Dense(512, activation='relu'),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(output_units, kernel_regularizer=tf.keras.regularizers.l2(5e-3), activation='softmax')\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  # Creating callbacks\n",
        "  checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='model/checkpoint/checkpoint_model.hdf5',\n",
        "                                                          save_best_only=True,\n",
        "                                                          verbose=1)\n",
        "\n",
        "  csvlogger_callback = tf.keras.callbacks.CSVLogger('model/csv_log/csvlog_model.log')\n",
        "\n",
        "  # Fit the model\n",
        "  history = model.fit(\n",
        "      train_generator,\n",
        "      validation_data=val_generator,\n",
        "      batch_size=128,\n",
        "      steps_per_epoch=num_train_generator // BATCH_SIZE,\n",
        "      validation_steps=num_val_generator // BATCH_SIZE,\n",
        "      epochs=30,\n",
        "      callbacks=[csvlogger_callback, checkpoint_callback],\n",
        "      verbose=1\n",
        "  )\n",
        "\n",
        "  return model, history"
      ],
      "metadata": {
        "id": "VdLDGXrjV0qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cont_model_icv3 = tf.keras.Sequential([\n",
        "#     icv3,\n",
        "#     tf.keras.layers.MaxPooling2D(),\n",
        "#     tf.keras.layers.Dense(512, activation='relu'),\n",
        "#     tf.keras.layers.Dropout(0.2),\n",
        "#     tf.keras.layers.Flatten(),\n",
        "#     tf.keras.layers.Dense(3, kernel_regularizer=tf.keras.regularizers.l2(5e-3), activation='softmax')\n",
        "# ])\n",
        "\n",
        "# fifth_model_icv3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "#                           loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "#                           metrics=['accuracy'])\n",
        "\n",
        "# # Creating callbacks\n",
        "# checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='model/checkpoint/checkpoint5th_3class_adam.hdf5',\n",
        "#                                                          save_best_only=True,\n",
        "#                                                          verbose=1)\n",
        "\n",
        "# csvlogger_callback = tf.keras.callbacks.CSVLogger('model/csv_log/csvlog5th_3class_adam.log')\n",
        "\n",
        "# # Fit the model\n",
        "# history_fifth_model_icv3 = fifth_model_icv3.fit(\n",
        "#     train_generator,\n",
        "#     validation_data=val_generator,\n",
        "#     batch_size=128,\n",
        "#     steps_per_epoch=num_train_generator // BATCH_SIZE,\n",
        "#     validation_steps=num_val_generator // BATCH_SIZE,\n",
        "#     epochs=30,\n",
        "#     callbacks=[csvlogger_callback, checkpoint_callback],\n",
        "#     verbose=1\n",
        "# )\n",
        "\n",
        "# fifth_model_icv3.save('model/model5th_icv3_adam.hdf5')"
      ],
      "metadata": {
        "id": "UshTKmaxl2Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cont_model, cont_history = train_model(11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tIA8aOwXE9c",
        "outputId": "b99963fa-c81c-4d17-c8ec-aab111fd4a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 1.7284 - accuracy: 0.4910\n",
            "Epoch 1: val_loss improved from inf to 0.93304, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 173s 2s/step - loss: 1.7284 - accuracy: 0.4910 - val_loss: 0.9330 - val_accuracy: 0.7233\n",
            "Epoch 2/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 1.0504 - accuracy: 0.6960\n",
            "Epoch 2: val_loss improved from 0.93304 to 0.81535, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 1.0504 - accuracy: 0.6960 - val_loss: 0.8154 - val_accuracy: 0.7679\n",
            "Epoch 3/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.9297 - accuracy: 0.7294\n",
            "Epoch 3: val_loss improved from 0.81535 to 0.74579, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.9297 - accuracy: 0.7294 - val_loss: 0.7458 - val_accuracy: 0.7946\n",
            "Epoch 4/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.8598 - accuracy: 0.7542\n",
            "Epoch 4: val_loss improved from 0.74579 to 0.70037, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.8598 - accuracy: 0.7542 - val_loss: 0.7004 - val_accuracy: 0.8070\n",
            "Epoch 5/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.8296 - accuracy: 0.7616\n",
            "Epoch 5: val_loss improved from 0.70037 to 0.68633, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.8296 - accuracy: 0.7616 - val_loss: 0.6863 - val_accuracy: 0.8093\n",
            "Epoch 6/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.7709 - accuracy: 0.7798\n",
            "Epoch 6: val_loss improved from 0.68633 to 0.66175, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.7709 - accuracy: 0.7798 - val_loss: 0.6618 - val_accuracy: 0.8153\n",
            "Epoch 7/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.7575 - accuracy: 0.7833\n",
            "Epoch 7: val_loss improved from 0.66175 to 0.63498, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.7575 - accuracy: 0.7833 - val_loss: 0.6350 - val_accuracy: 0.8240\n",
            "Epoch 8/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.7250 - accuracy: 0.7942\n",
            "Epoch 8: val_loss did not improve from 0.63498\n",
            "68/68 [==============================] - 152s 2s/step - loss: 0.7250 - accuracy: 0.7942 - val_loss: 0.6395 - val_accuracy: 0.8254\n",
            "Epoch 9/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.7109 - accuracy: 0.7980\n",
            "Epoch 9: val_loss improved from 0.63498 to 0.61993, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.7109 - accuracy: 0.7980 - val_loss: 0.6199 - val_accuracy: 0.8309\n",
            "Epoch 10/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.6908 - accuracy: 0.7995\n",
            "Epoch 10: val_loss improved from 0.61993 to 0.60261, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 155s 2s/step - loss: 0.6908 - accuracy: 0.7995 - val_loss: 0.6026 - val_accuracy: 0.8359\n",
            "Epoch 11/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.6775 - accuracy: 0.8080\n",
            "Epoch 11: val_loss improved from 0.60261 to 0.59073, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 154s 2s/step - loss: 0.6775 - accuracy: 0.8080 - val_loss: 0.5907 - val_accuracy: 0.8410\n",
            "Epoch 12/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.6424 - accuracy: 0.8211\n",
            "Epoch 12: val_loss improved from 0.59073 to 0.58243, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 154s 2s/step - loss: 0.6424 - accuracy: 0.8211 - val_loss: 0.5824 - val_accuracy: 0.8405\n",
            "Epoch 13/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.6496 - accuracy: 0.8145\n",
            "Epoch 13: val_loss did not improve from 0.58243\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.6496 - accuracy: 0.8145 - val_loss: 0.5866 - val_accuracy: 0.8373\n",
            "Epoch 14/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.6279 - accuracy: 0.8209\n",
            "Epoch 14: val_loss improved from 0.58243 to 0.56796, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.6279 - accuracy: 0.8209 - val_loss: 0.5680 - val_accuracy: 0.8382\n",
            "Epoch 15/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.6166 - accuracy: 0.8269\n",
            "Epoch 15: val_loss did not improve from 0.56796\n",
            "68/68 [==============================] - 154s 2s/step - loss: 0.6166 - accuracy: 0.8269 - val_loss: 0.5750 - val_accuracy: 0.8378\n",
            "Epoch 16/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.6112 - accuracy: 0.8265\n",
            "Epoch 16: val_loss improved from 0.56796 to 0.56394, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.6112 - accuracy: 0.8265 - val_loss: 0.5639 - val_accuracy: 0.8433\n",
            "Epoch 17/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5842 - accuracy: 0.8350\n",
            "Epoch 17: val_loss improved from 0.56394 to 0.56196, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 152s 2s/step - loss: 0.5842 - accuracy: 0.8350 - val_loss: 0.5620 - val_accuracy: 0.8405\n",
            "Epoch 18/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5816 - accuracy: 0.8341\n",
            "Epoch 18: val_loss improved from 0.56196 to 0.55172, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 152s 2s/step - loss: 0.5816 - accuracy: 0.8341 - val_loss: 0.5517 - val_accuracy: 0.8447\n",
            "Epoch 19/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5700 - accuracy: 0.8365\n",
            "Epoch 19: val_loss improved from 0.55172 to 0.54765, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 152s 2s/step - loss: 0.5700 - accuracy: 0.8365 - val_loss: 0.5476 - val_accuracy: 0.8520\n",
            "Epoch 20/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5644 - accuracy: 0.8404\n",
            "Epoch 20: val_loss improved from 0.54765 to 0.54740, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 152s 2s/step - loss: 0.5644 - accuracy: 0.8404 - val_loss: 0.5474 - val_accuracy: 0.8488\n",
            "Epoch 21/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5559 - accuracy: 0.8421\n",
            "Epoch 21: val_loss improved from 0.54740 to 0.53900, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.5559 - accuracy: 0.8421 - val_loss: 0.5390 - val_accuracy: 0.8483\n",
            "Epoch 22/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5502 - accuracy: 0.8383\n",
            "Epoch 22: val_loss improved from 0.53900 to 0.53157, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 156s 2s/step - loss: 0.5502 - accuracy: 0.8383 - val_loss: 0.5316 - val_accuracy: 0.8511\n",
            "Epoch 23/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5409 - accuracy: 0.8410\n",
            "Epoch 23: val_loss improved from 0.53157 to 0.52669, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.5409 - accuracy: 0.8410 - val_loss: 0.5267 - val_accuracy: 0.8479\n",
            "Epoch 24/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5258 - accuracy: 0.8501\n",
            "Epoch 24: val_loss improved from 0.52669 to 0.51839, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.5258 - accuracy: 0.8501 - val_loss: 0.5184 - val_accuracy: 0.8529\n",
            "Epoch 25/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5201 - accuracy: 0.8506\n",
            "Epoch 25: val_loss did not improve from 0.51839\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.5201 - accuracy: 0.8506 - val_loss: 0.5316 - val_accuracy: 0.8502\n",
            "Epoch 26/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5023 - accuracy: 0.8561\n",
            "Epoch 26: val_loss did not improve from 0.51839\n",
            "68/68 [==============================] - 157s 2s/step - loss: 0.5023 - accuracy: 0.8561 - val_loss: 0.5273 - val_accuracy: 0.8516\n",
            "Epoch 27/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5194 - accuracy: 0.8544\n",
            "Epoch 27: val_loss did not improve from 0.51839\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.5194 - accuracy: 0.8544 - val_loss: 0.5202 - val_accuracy: 0.8516\n",
            "Epoch 28/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5119 - accuracy: 0.8503\n",
            "Epoch 28: val_loss improved from 0.51839 to 0.50856, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.5119 - accuracy: 0.8503 - val_loss: 0.5086 - val_accuracy: 0.8562\n",
            "Epoch 29/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.4942 - accuracy: 0.8600\n",
            "Epoch 29: val_loss did not improve from 0.50856\n",
            "68/68 [==============================] - 152s 2s/step - loss: 0.4942 - accuracy: 0.8600 - val_loss: 0.5221 - val_accuracy: 0.8506\n",
            "Epoch 30/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.4825 - accuracy: 0.8627\n",
            "Epoch 30: val_loss did not improve from 0.50856\n",
            "68/68 [==============================] - 153s 2s/step - loss: 0.4825 - accuracy: 0.8627 - val_loss: 0.5099 - val_accuracy: 0.8621\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cont_model.save(\"model/cont_model_1\")\n",
        "\n",
        "!zip -r ./cont_model_1.zip model\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"./cont_model_1.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "k44y20HYXE7X",
        "outputId": "7021e86b-fc0a-4dc3-90cb-4c024218885e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: model/cont_model_1/assets\n",
            "  adding: model/ (stored 0%)\n",
            "  adding: model/checkpoint/ (stored 0%)\n",
            "  adding: model/checkpoint/checkpoint_model.hdf5 (deflated 8%)\n",
            "  adding: model/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: model/csv_log/ (stored 0%)\n",
            "  adding: model/csv_log/csvlog_model.log (deflated 52%)\n",
            "  adding: model/cont_model_1/ (stored 0%)\n",
            "  adding: model/cont_model_1/assets/ (stored 0%)\n",
            "  adding: model/cont_model_1/keras_metadata.pb (deflated 96%)\n",
            "  adding: model/cont_model_1/saved_model.pb (deflated 92%)\n",
            "  adding: model/cont_model_1/variables/ (stored 0%)\n",
            "  adding: model/cont_model_1/variables/variables.index (deflated 76%)\n",
            "  adding: model/cont_model_1/variables/variables.data-00000-of-00001 (deflated 7%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_71bdce03-06bd-41d2-a62b-10a29e6e4ab0\", \"cont_model_1.zip\", 186885184)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WUR1HjxNXE5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First EfficientNetB2 Model"
      ],
      "metadata": {
        "id": "XUUgB9O93wRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute this when loading model from previous checkpoint\n",
        "model_path = \"./model/checkpoint/checkpoint_model.hdf5\"\n",
        "\n",
        "model_efnetb2_1 = tf.keras.models.load_model(model_path)"
      ],
      "metadata": {
        "id": "ndTHXkaYdQ46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EfficientNet doesn't use rescale as its already in their preprocessing layer\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "# IMG_WIDTH, IMG_HEIGHT = 224, 224  # For EfficientNetB0\n",
        "IMG_WIDTH, IMG_HEIGHT = 260, 260  # For EfficientNetB2\n",
        "\n",
        "def data_generator_func(data_dir, img_width, img_height, batch_size):\n",
        "    train_datagen = ImageDataGenerator(#rescale=1./255,\n",
        "                                      rotation_range=60,\n",
        "                                      width_shift_range=0.2,\n",
        "                                      height_shift_range=0.2,\n",
        "                                      shear_range=0.2,\n",
        "                                      zoom_range=0.2,\n",
        "                                      horizontal_flip=True,\n",
        "                                      fill_mode='nearest',\n",
        "                                      validation_split=0.2)\n",
        "    \n",
        "    train_generator = train_datagen.flow_from_directory(directory=data_dir,\n",
        "                                                        target_size=(img_width, img_height),\n",
        "                                                        batch_size=batch_size,\n",
        "                                                        shuffle=True,\n",
        "                                                        class_mode='categorical',\n",
        "                                                        subset='training')\n",
        "    \n",
        "    val_datagen = ImageDataGenerator(#rescale=1./255,\n",
        "                                    validation_split=0.2)\n",
        "    \n",
        "    val_generator = val_datagen.flow_from_directory(directory=data_dir,\n",
        "                                                    target_size=(img_width, img_height),\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    subset='validation')\n",
        "    \n",
        "    return train_generator, val_generator\n",
        "\n",
        "train_generator, val_generator = data_generator_func('subset_images', IMG_WIDTH, IMG_HEIGHT, BATCH_SIZE)\n",
        "#train_generator, val_generator = data_generator_func(DATA_DIR, IMG_WIDTH, IMG_HEIGHT, BATCH_SIZE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dO3eTjU6wOQ",
        "outputId": "4bec7416-40fa-4c91-b4f9-92f36d40fb2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8800 images belonging to 11 classes.\n",
            "Found 2200 images belonging to 11 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from tensorflow.keras.applications import EfficientNetB2"
      ],
      "metadata": {
        "id": "FO98OyID6XpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SHAPE = (260, 260, 3)\n",
        "BATCH_SIZE = 128\n",
        "efnetb2 = tf.keras.applications.EfficientNetB2(input_shape=INPUT_SHAPE, include_top=False)\n",
        "\n",
        "def create_model(base_model, num_classes=11):\n",
        "  # Freezing EfficientNet layer\n",
        "  base_model.trainable = False\n",
        "\n",
        "  x = base_model.output\n",
        "  x = tf.keras.layers.MaxPooling2D()(x)\n",
        "  x = tf.keras.layers.Dense(units=512, activation='relu')(x)\n",
        "  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  x = tf.keras.layers.Flatten()(x)\n",
        "  outputs = tf.keras.layers.Dense(units=num_classes, kernel_regularizer=tf.keras.regularizers.l2(5e-3), activation='softmax')(x)\n",
        "\n",
        "  model = tf.keras.Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "  # Compiling model\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "9SCu9eVtXE3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_efnetb2_1 = create_model(efnetb2)\n",
        "\n",
        "# Creating callbacks\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='model/checkpoint/checkpoint_model.hdf5',\n",
        "                                                          save_best_only=True,\n",
        "                                                          verbose=1)\n",
        "\n",
        "csvlogger_callback = tf.keras.callbacks.CSVLogger('model/csv_log/csvlog_model.log')\n",
        "\n",
        "history_efnetb2_1 = model_efnetb2_1.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=30,\n",
        "    steps_per_epoch=num_train_generator // BATCH_SIZE,\n",
        "    validation_steps=num_val_generator // BATCH_SIZE,\n",
        "    callbacks=[csvlogger_callback, checkpoint_callback, tensorboard_callback(\"efnet_ver1\")],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_WU9MUxl2Lj",
        "outputId": "115e8a48-275e-4a98-e4c2-d509cdd25b5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: ./model/tfboardefnet_ver1/20220531-095348\n",
            "Epoch 1/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 1.6151 - accuracy: 0.5180\n",
            "Epoch 1: val_loss improved from inf to 0.76246, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 185s 3s/step - loss: 1.6151 - accuracy: 0.5180 - val_loss: 0.7625 - val_accuracy: 0.7941\n",
            "Epoch 2/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.8646 - accuracy: 0.7561\n",
            "Epoch 2: val_loss improved from 0.76246 to 0.60942, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 177s 3s/step - loss: 0.8646 - accuracy: 0.7561 - val_loss: 0.6094 - val_accuracy: 0.8410\n",
            "Epoch 3/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.7327 - accuracy: 0.7943\n",
            "Epoch 3: val_loss improved from 0.60942 to 0.54667, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 168s 2s/step - loss: 0.7327 - accuracy: 0.7943 - val_loss: 0.5467 - val_accuracy: 0.8603\n",
            "Epoch 4/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.6785 - accuracy: 0.8124\n",
            "Epoch 4: val_loss improved from 0.54667 to 0.50503, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 169s 2s/step - loss: 0.6785 - accuracy: 0.8124 - val_loss: 0.5050 - val_accuracy: 0.8755\n",
            "Epoch 5/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.6299 - accuracy: 0.8299\n",
            "Epoch 5: val_loss improved from 0.50503 to 0.48556, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 175s 3s/step - loss: 0.6299 - accuracy: 0.8299 - val_loss: 0.4856 - val_accuracy: 0.8824\n",
            "Epoch 6/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5947 - accuracy: 0.8401\n",
            "Epoch 6: val_loss improved from 0.48556 to 0.47347, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 175s 3s/step - loss: 0.5947 - accuracy: 0.8401 - val_loss: 0.4735 - val_accuracy: 0.8883\n",
            "Epoch 7/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5687 - accuracy: 0.8514\n",
            "Epoch 7: val_loss improved from 0.47347 to 0.45748, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 167s 2s/step - loss: 0.5687 - accuracy: 0.8514 - val_loss: 0.4575 - val_accuracy: 0.8934\n",
            "Epoch 8/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5443 - accuracy: 0.8559\n",
            "Epoch 8: val_loss improved from 0.45748 to 0.45309, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 167s 2s/step - loss: 0.5443 - accuracy: 0.8559 - val_loss: 0.4531 - val_accuracy: 0.8892\n",
            "Epoch 9/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5113 - accuracy: 0.8691\n",
            "Epoch 9: val_loss improved from 0.45309 to 0.43842, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 175s 3s/step - loss: 0.5113 - accuracy: 0.8691 - val_loss: 0.4384 - val_accuracy: 0.8934\n",
            "Epoch 10/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5085 - accuracy: 0.8647\n",
            "Epoch 10: val_loss improved from 0.43842 to 0.42108, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 174s 3s/step - loss: 0.5085 - accuracy: 0.8647 - val_loss: 0.4211 - val_accuracy: 0.8957\n",
            "Epoch 11/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.4890 - accuracy: 0.8730\n",
            "Epoch 11: val_loss did not improve from 0.42108\n",
            "68/68 [==============================] - 166s 2s/step - loss: 0.4890 - accuracy: 0.8730 - val_loss: 0.4234 - val_accuracy: 0.8957\n",
            "Epoch 12/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.4719 - accuracy: 0.8803\n",
            "Epoch 12: val_loss improved from 0.42108 to 0.42033, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 166s 2s/step - loss: 0.4719 - accuracy: 0.8803 - val_loss: 0.4203 - val_accuracy: 0.9026\n",
            "Epoch 13/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.4614 - accuracy: 0.8824\n",
            "Epoch 13: val_loss improved from 0.42033 to 0.41530, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 167s 2s/step - loss: 0.4614 - accuracy: 0.8824 - val_loss: 0.4153 - val_accuracy: 0.8998\n",
            "Epoch 14/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.4425 - accuracy: 0.8809\n",
            "Epoch 14: val_loss improved from 0.41530 to 0.41246, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 175s 3s/step - loss: 0.4425 - accuracy: 0.8809 - val_loss: 0.4125 - val_accuracy: 0.9017\n",
            "Epoch 15/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.4262 - accuracy: 0.8866\n",
            "Epoch 15: val_loss improved from 0.41246 to 0.40596, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 173s 3s/step - loss: 0.4262 - accuracy: 0.8866 - val_loss: 0.4060 - val_accuracy: 0.9030\n",
            "Epoch 16/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.4150 - accuracy: 0.8902\n",
            "Epoch 16: val_loss improved from 0.40596 to 0.40291, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 173s 3s/step - loss: 0.4150 - accuracy: 0.8902 - val_loss: 0.4029 - val_accuracy: 0.9003\n",
            "Epoch 17/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.4151 - accuracy: 0.8954\n",
            "Epoch 17: val_loss improved from 0.40291 to 0.39611, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 166s 2s/step - loss: 0.4151 - accuracy: 0.8954 - val_loss: 0.3961 - val_accuracy: 0.9021\n",
            "Epoch 18/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3986 - accuracy: 0.8997\n",
            "Epoch 18: val_loss improved from 0.39611 to 0.39445, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 167s 2s/step - loss: 0.3986 - accuracy: 0.8997 - val_loss: 0.3945 - val_accuracy: 0.9030\n",
            "Epoch 19/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3926 - accuracy: 0.8990\n",
            "Epoch 19: val_loss did not improve from 0.39445\n",
            "68/68 [==============================] - 172s 3s/step - loss: 0.3926 - accuracy: 0.8990 - val_loss: 0.3957 - val_accuracy: 0.9030\n",
            "Epoch 20/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3820 - accuracy: 0.9019\n",
            "Epoch 20: val_loss improved from 0.39445 to 0.39401, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 173s 3s/step - loss: 0.3820 - accuracy: 0.9019 - val_loss: 0.3940 - val_accuracy: 0.9007\n",
            "Epoch 21/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3757 - accuracy: 0.9065\n",
            "Epoch 21: val_loss improved from 0.39401 to 0.38515, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 172s 3s/step - loss: 0.3757 - accuracy: 0.9065 - val_loss: 0.3852 - val_accuracy: 0.9072\n",
            "Epoch 22/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3730 - accuracy: 0.9060\n",
            "Epoch 22: val_loss did not improve from 0.38515\n",
            "68/68 [==============================] - 168s 2s/step - loss: 0.3730 - accuracy: 0.9060 - val_loss: 0.3897 - val_accuracy: 0.9044\n",
            "Epoch 23/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3522 - accuracy: 0.9110\n",
            "Epoch 23: val_loss did not improve from 0.38515\n",
            "68/68 [==============================] - 165s 2s/step - loss: 0.3522 - accuracy: 0.9110 - val_loss: 0.3887 - val_accuracy: 0.9017\n",
            "Epoch 24/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3556 - accuracy: 0.9114\n",
            "Epoch 24: val_loss improved from 0.38515 to 0.38135, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 165s 2s/step - loss: 0.3556 - accuracy: 0.9114 - val_loss: 0.3813 - val_accuracy: 0.9085\n",
            "Epoch 25/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3530 - accuracy: 0.9111\n",
            "Epoch 25: val_loss improved from 0.38135 to 0.38018, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 165s 2s/step - loss: 0.3530 - accuracy: 0.9111 - val_loss: 0.3802 - val_accuracy: 0.9076\n",
            "Epoch 26/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3341 - accuracy: 0.9158\n",
            "Epoch 26: val_loss improved from 0.38018 to 0.37834, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 173s 3s/step - loss: 0.3341 - accuracy: 0.9158 - val_loss: 0.3783 - val_accuracy: 0.9076\n",
            "Epoch 27/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3409 - accuracy: 0.9131\n",
            "Epoch 27: val_loss did not improve from 0.37834\n",
            "68/68 [==============================] - 165s 2s/step - loss: 0.3409 - accuracy: 0.9131 - val_loss: 0.3795 - val_accuracy: 0.9072\n",
            "Epoch 28/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3238 - accuracy: 0.9180\n",
            "Epoch 28: val_loss improved from 0.37834 to 0.37140, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 165s 2s/step - loss: 0.3238 - accuracy: 0.9180 - val_loss: 0.3714 - val_accuracy: 0.9090\n",
            "Epoch 29/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3169 - accuracy: 0.9232\n",
            "Epoch 29: val_loss improved from 0.37140 to 0.36618, saving model to model/checkpoint/checkpoint_model.hdf5\n",
            "68/68 [==============================] - 165s 2s/step - loss: 0.3169 - accuracy: 0.9232 - val_loss: 0.3662 - val_accuracy: 0.9104\n",
            "Epoch 30/30\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3220 - accuracy: 0.9181\n",
            "Epoch 30: val_loss did not improve from 0.36618\n",
            "68/68 [==============================] - 166s 2s/step - loss: 0.3220 - accuracy: 0.9181 - val_loss: 0.3758 - val_accuracy: 0.9095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qFFsTi-oe036"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r ./model_efnetb2_1.zip model\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"./model_efnetb2_1.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "Sel1d0Ax-p8l",
        "outputId": "117ca38e-820f-415e-c4e2-e30251bd9883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: model/ (stored 0%)\n",
            "  adding: model/checkpoint/ (stored 0%)\n",
            "  adding: model/checkpoint/checkpoint_model.hdf5 (deflated 10%)\n",
            "  adding: model/checkpoint/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: model/tfboardefnet_ver1/ (stored 0%)\n",
            "  adding: model/tfboardefnet_ver1/20220531-095348/ (stored 0%)\n",
            "  adding: model/tfboardefnet_ver1/20220531-095348/validation/ (stored 0%)\n",
            "  adding: model/tfboardefnet_ver1/20220531-095348/validation/events.out.tfevents.1653990991.92391deb65b5.75.2.v2 (deflated 77%)\n",
            "  adding: model/tfboardefnet_ver1/20220531-095348/train/ (stored 0%)\n",
            "  adding: model/tfboardefnet_ver1/20220531-095348/train/events.out.tfevents.1653990830.92391deb65b5.75.1.v2 (deflated 93%)\n",
            "  adding: model/tfboardefnet_ver1/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: model/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: model/csv_log/ (stored 0%)\n",
            "  adding: model/csv_log/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: model/csv_log/csvlog_model.log (deflated 53%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b7aaf312-f304-4510-bcac-6d6bbb685bf9\", \"model_efnetb2_1.zip\", 37813041)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Rz3eEAlA-p6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TensorBoard Visualize and Playground"
      ],
      "metadata": {
        "id": "a4KJzSvF4OXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "def tensorboard_callback(experiment_name):\n",
        "  log_dir = \"./model/tfboard\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  \n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
        "\n",
        "  return tensorboard_callback"
      ],
      "metadata": {
        "id": "ROXEfJ-b4R_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator.samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0_FOpG14TWK",
        "outputId": "5318ae5d-9b9d-4974-a526-43320a7820f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8800"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip files\n",
        "!unzip model_efnet_1.zip"
      ],
      "metadata": {
        "id": "bIvAaRhm4TUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50228d81-3736-453a-a727-97331e328ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  model_efnet_1.zip\n",
            "   creating: model/\n",
            "   creating: model/checkpoint/\n",
            "  inflating: model/checkpoint/checkpoint_model.hdf5  \n",
            "   creating: model/tfboardefnet_ver1/\n",
            "   creating: model/tfboardefnet_ver1/20220529-132959/\n",
            "   creating: model/tfboardefnet_ver1/20220529-132959/train/\n",
            "  inflating: model/tfboardefnet_ver1/20220529-132959/train/events.out.tfevents.1653831002.3fcddb13b51f.85.0.v2  \n",
            "   creating: model/tfboardefnet_ver1/20220529-133135/\n",
            "   creating: model/tfboardefnet_ver1/20220529-133135/validation/\n",
            "  inflating: model/tfboardefnet_ver1/20220529-133135/validation/events.out.tfevents.1653831273.3fcddb13b51f.85.2.v2  \n",
            "   creating: model/tfboardefnet_ver1/20220529-133135/train/\n",
            "  inflating: model/tfboardefnet_ver1/20220529-133135/train/events.out.tfevents.1653831097.3fcddb13b51f.85.1.v2  \n",
            "   creating: model/.ipynb_checkpoints/\n",
            "   creating: model/csv_log/\n",
            "  inflating: model/csv_log/csvlog_model.log  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6SudIDDv4TSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uCPW5KFl4TQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1ZVM9-lc4TOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Q3BXke7f4TMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VAG7lPuV4TJq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}